<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://benknoble.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benknoble.github.io/" rel="alternate" type="text/html" /><updated>2024-11-13T00:14:41+00:00</updated><id>https://benknoble.github.io/feed.xml</id><title type="html">Junk Drawer</title><subtitle>For all those little papers scattered across your desk</subtitle><author><name>benknoble</name></author><entry><title type="html">Perspective on software development models</title><link href="https://benknoble.github.io/blog/2024/11/11/git-branch-trunk-perspective/" rel="alternate" type="text/html" title="Perspective on software development models" /><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/11/11/git-branch-trunk-perspective</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/11/11/git-branch-trunk-perspective/"><![CDATA[<p>If I believe <a href="/about/#me">simple questions have complex answers</a>, then it’s no surprise that I have a complicated take on the following
question: branch-based or trunk-based development? Which is better, and why?</p>

<p>As usual, though, <em>we’re asking the wrong questions.</em> For spoilers, jump to the
<a href="#conclusion">conclusion</a>.</p>

<h2 id="impetus-opinion-and-advantage-without-mechanism-or-goal">Impetus: opinion and advantage without mechanism or goal</h2>

<p>Part of this post is a reply to posts like <a href="https://trishagee.com/2023/05/29/why-i-prefer-trunk-based-development/">Trisha Gee’s “Why I prefer
trunk-based
development”</a>.
If you don’t care for musings on that, skip to <a href="#analysis">the beginning of my
analysis</a> for the rest of the post, which examines the tradeoffs of
branch-based and trunk-based development in light of 5 pillars.</p>

<p>Gee outshines many of her peers with her titular framing: the article explores
<em>her</em> preferences based on <em>her</em> experience. Too many answers to the question
of “to branch or not to branch” want to convince you of their absolute
correctness, shades of gray be damned. And if that kind of extreme conviction
sets off your spidey senses, it’s time to double-check: are the authors of such
answers trying to sell you something?</p>

<p>Others repeat tired claims, often without evidence. This is cargo-cult
programming and should be approached with the same skepticism: <a href="https://chelseatroy.com/2022/04/18/best-practice-is-not-a-reason-to-do-something/">“Best Practice”
is not a reason to do something</a>.</p>

<p>Gee stands out, then, for speaking personally and from professional experience.
Her introduction winds you up for a “when I was at X and we did Y, we found Z”
tale. These stories form the basis of mature programmer thinking: I learn from
your experience and can better judge appropriate tradeoffs thanks to your lived
and earned wisdom. <strong>From Gee’s analysis of tradeoffs, I hoped to learn and to
be better equipped to analyze my own.</strong> The results might be measurable or gut
feelings, and we would learn from both.</p>

<p>The main article falls flat.</p>

<p>Teed up for story time, we find instead a collection of opinion presented as
fact. This being the internet, of course we find opinion: there must be memories
that inform Gee’s opinions (or else we are reading another cargo-cult piece, and
I want to assume positive intent of Gee, whom I don’t know). We can’t extricate
our opinions from the experiences that shape us. Thus Gee speaks from experience
but shares none of it<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">1</a></sup>. For example, Gee claims</p>

<blockquote>
  <p>Integrating small changes regularly into your code is usually less painful
than a big merge at the end of a longer period of time.</p>
</blockquote>

<p>This is probably true (“usually”), but comes with none of the analysis that lets
us discern when or why. We’ll examine specific claims later; for now, we are
disappointed in yet another “this is the way” string of paragraphs.</p>

<p>What are we missing? We have a set of opinions which are presumably backed by
the author’s experience and which provide, according to her, some advantage. We
lack</p>

<ul>
  <li>the actual mechanism by which these opinions are practiced, and</li>
  <li>a set of goals or desires against which we can judge the tradeoffs of
different approaches and their effects.</li>
</ul>

<p>We don’t seek analysis to ground our precious art in hard-to-find evidence<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">2</a></sup>
or to participate in <a href="https://pluralistic.net/2024/10/29/hobbesian-slop/">empiricism
washing</a><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">3</a></sup>; rather, we aim
to derive from shared opinions lessons we apply to our own situations. We
welcome opinion. For claims of advantage, we insist on analysis in order to
integrate that opinion.</p>

<p>There is other material about the mechanisms of branch- and trunk-based
workflows. <a href="https://git-scm.com/book/en/v2/Distributed-Git-Distributed-Workflows">Pro Git even covers
some</a>, as
does <a href="https://git-scm.com/docs/gitworkflows"><code class="language-plaintext highlighter-rouge">git help workflows</code></a>. Still I want
to mix discussion of tradeoff with implementation mechanism: I find this
clarifies my thoughts.</p>

<p><a id="analysis"></a>We’ll take Gee’s 5 points for trunk-based workflows and
distill both mechanism and tradeoff for branch- and trunk-based workflows. By
the end, you’ll see how I view the wide world of Git use.</p>

<h2 id="speed-and-efficiency">Speed and Efficiency</h2>

<p>Gee claims that trunk-based development means code is integrated more quickly
and more efficiently:</p>

<blockquote>
  <p>This model allows for quicker integrations and fewer merge conflicts. […] It
might seem fast to develop your fixes and improvements on a separate branch
that isn’t impacted by other developers’ changes, but you still have to pay
that cost at some point. Integrating small changes regularly into your code is
usually less painful than a big merge at the end of a longer period of time.</p>
</blockquote>

<p>Here “integrated” code is code that has been combined together to serve some
purpose. Typically it is independently developed code, often for disparate
features or fixes.</p>

<p>Because we lack mechanism, we’re going to take trunk-based development’s
mechanism to mean something like</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>while ! git push; do
    git pull --rebase
done
</code></pre></div></div>

<p>In other words, rebase your single shared branch locally until you are able to
win the push race—after all, if I push first, Git will reject your
non-fast-forward push. (You may merge instead of rebase, but that becomes messy
for no productive reason.)</p>

<p>What is branch-based development’s equivalent workflow? It depends on your team:
the patterns I’ve seen are:</p>
<ul>
  <li>On-demand: Don’t integrate the upstream branch into the topic branch unless
there are conflicts that need resolved or upstream features that the topic
wants. Only then perform a rebase or merge, as desired. Many open-source
projects frown on merging from upstream when it is avoidable: prefer to base
your work on the code that it needs, meaning you will need to rebase if you
need new work.</li>
  <li>Continuous: Integrate the upstream branch constantly, typically via merge. I
see this when “Require branches to be up to date before merging” is enabled
for a repository, and especially when the default (or only) merge strategy is
<a href="/blog/2024/08/02/github-squash/">squashing</a>. In the corporate
environments I’ve seen this in, pushing otherwise unnecessary merges which
clutter the PR is fine since they will disappear anyway.</li>
</ul>

<p>I personally keep to a middle-ground of rebasing on-demand, but more often than
“only when there are conflicts” would suggest: in centralized workflows, I try
to keep branches with PRs based on the latest main branch. In distributed
workflows, I’m less picky about updating until I start a new version of a
branch.</p>

<p>Let’s assume both workflows follow good commit hygiene (small, focused, frequent
commits) aside from debatably-unnecessary merges. Now we can examine tradeoffs:</p>

<ol>
  <li><em>All</em> integrators have to deal with conflicts eventually.</li>
  <li>Trunk-based developers and continuous branch integrators integrate code more
often by nature of the way they operate.</li>
  <li>Assuming trunk-based developers push as soon as they have a working commit
and that they work in a team, they must have to pull frequently. If they
don’t, they push far more frequently than any of their teammates, who <em>do</em>
have to pull frequently. This can be a source of friction. In solo mode, the
push is never rejected, which makes this strategy especially appealing.</li>
  <li>On-demand integrators typically have more commits to rebase or merge when
integrating because of the frequency at which it happens, and we all agree
that more commits means more probability of conflicts.</li>
  <li>On the flip side, making PR review and merge high-priority in a branch-based
workflow means few branches live long enough to deal with complex conflicts.
This might balance out against frequent integrations if a PR is quickly
merged (“near-trunk-based”) without conflicts: on-demand saves some effort
here.</li>
</ol>

<h2 id="greater-code-stability">Greater Code Stability</h2>

<p>Gee claims that trunk-based development</p>

<blockquote>
  <p>encourages frequent commits, which leads to smaller and more manageable
changes. […] In the branching model, large and infrequent merges can introduce
bugs that are hard to identify and resolve due to the sheer size of the
changes.</p>
</blockquote>

<p>Gee argues that more commits (by proxy of “more time”) lead to a higher chance
of conflicts, which I agree with above and for which I suggested a mitigation.
Then she constructs a <a id="strawman"></a><strong>strawman</strong> version of the branching
model: to compare <em>well-run</em> trunk-based development, we should also use
<em>well-run</em> branch-based development, in which ready branches are frequently
reviewed and either merged or rejected<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">4</a></sup>.</p>

<p>In a well-run branch model, buggy merges are infrequent. Said another way, a
trunk-based developer who hasn’t pulled in a while and creates a large merge
will have the same problems.</p>

<blockquote>
  <p>By frequently pulling in the other developers’ changes, and frequently pushing
small changes of working code, we know the codebase is stable and working.</p>
</blockquote>

<p>This actually depends on stable testing, as Gee writes, and is <strong>independent of
workflow.</strong> In sum, code stability depends more on practices exterior to the
particular workflow than on branch vs. trunk, personal experience and observed
correlations notwithstanding.</p>

<h2 id="enhanced-team-collaboration">Enhanced Team Collaboration</h2>

<p>Gee writes:</p>

<blockquote>
  <p>If you’re all working on your own branches, you are not collaborating. You are
competing. To see who can get their code in fastest. To avoid being stomped on
by someone else’s code changes.</p>
</blockquote>

<p>And yet, as the <code class="language-plaintext highlighter-rouge">while</code> loop demonstrates above, trunk-based developers might
stomp on each other, too. <strong>This is independent of workflow.</strong></p>

<p>This is often a social problem rather than a technical one; that is, I need to
speak with my team about the areas we’re working on and how to organize them.
Resolving that resolves many issues.</p>

<blockquote>
  <p>If you’re all working on the same branch, you tend to have a better awareness
of the changes being made. This approach fosters greater team collaboration
and knowledge sharing. In contrast, branching can create a siloed work
environment where you’re all working independently, leading to knowledge gaps
within the team.</p>
</blockquote>

<p>Conversely, if most of a small team reviews every PR, we still share knowledge.
And for large projects with many subsystems (such as the Rust compiler),
de-siloing knowledge happens via RFC and commit hygiene, not (necessarily) by
reading every pulled commit. <strong>This problem is independent of workflow.</strong></p>

<p>A few actual tradeoffs that are workflow-relevant:</p>

<ol>
  <li>Distributed development teams, whether open source or private, may not be
able to accept trunk-based contributions from community for security or
stability reasons: giving the internet commit rights to your deployment
branch is a bad idea. The Rhombus project is driven primarily by PRs but
cannot for stability, give commit rights to everyone.</li>
  <li>Conversely, small teams with no or few other collaborators can commit
directly and reserve PR workflows for those few outside contributors. The
main Racket repository looks like this (in spite of many external
contributors), as do many other open source projects with active development
communities who don’t mind core devs committing directly.</li>
</ol>

<h2 id="improved-continuous-integration-and-delivery-cicd-practices">Improved Continuous Integration and Delivery (CI/CD) Practices</h2>

<p>Gee claims trunk-based development helps CI/CD because</p>

<blockquote>
  <p>Any failures there are seen and addressed promptly, reducing the risk of nasty
failures. It’s usually easy to track down which changes caused the problem. If
the issue can’t be fixed immediately, you can back the specific changes that
caused it.</p>
</blockquote>

<p>As may by now be clear, <strong>this is independent of workflow.</strong> Tools like <code class="language-plaintext highlighter-rouge">git
blame</code> and <code class="language-plaintext highlighter-rouge">git bisect</code> are essential to tracking changes, and CI on PRs can
catch them before they are integrated. A CI system would need to warn me loudly
that a push caused a failure for me to notice: with branch-based PRs, I am
frequently looking at the status checks on a GitHub page (for example). That
doesn’t mean a trunk-based CI system couldn’t be noisy! It’s a matter of tooling
and choice, not inherent advantage.</p>

<blockquote>
  <p>It’s at merge (or rebase) time that you start to see any integration issues.
All those tests you were running on your branch “in CI” were not testing any
kind of integration at all.</p>
</blockquote>

<p>This is certainly true and is one of the main arguments for “Require branches to
be up to date before merging.” CI typically also runs on the main branch,
however.</p>

<p>I do have experience with continuous <em>deployment</em> becoming confusing with branch
models: fundamentally it’s a tooling failure, however, because we don’t have
per-PR environments. So to test a deployed PR I might be stomping on some
other PR’s dev or QA environment.</p>

<p>Heavily regulated environments (such as my current workplace) may not be able to
treat each push as a deployment to customers: instead, in some domains we have
to be careful about when we release even if we merge frequently. For our
internal customers, release on push often works fine.</p>

<h2 id="reduced-technical-debt">Reduced Technical Debt</h2>

<p>Gee claims that “merge hell” contributes to technical debt, and I agree: it
creates temptation for quick resolution rather than careful design. All
technical debt is tradeoff, though, and we usually choose to accept some.</p>

<blockquote>
  <p>you may […] accept suggestions from your IDE that resolve the merge but that
you don’t fully understand.</p>
</blockquote>

<p>Not understanding code you produced via tool <strong>is independent of workflow.</strong>
Stop doing that. (Code review <em>may</em> catch this problem but is not a failure-free
safety net. See the “swiss cheese” model of code review.)</p>

<blockquote>
  <p>With trunk-based development, frequent merges and smaller changes make it
easier to manage and reduce the build-up of technical debt.</p>
</blockquote>

<p>Replace “trunk-based” with “branch-based”: does the sentence still ring true? I
think so. <strong>This problem is independent of workflow.</strong> Our assumptions of commit
hygiene support the claim of independence.</p>

<h2 id="conclusion">Conclusion</h2>

<p>See if this sounds right to you:</p>

<blockquote>
  <p>[Software development] requires a mindset, a culture, within the development
team. You need to frequently merge in other developers’ changes into your own
code. You need to commit small changes, frequently, which requires you to only
change small sections of the code and make incremental changes, something
which can be a difficult habit to get used to. Pair programming, comprehensive
automated testing, and maybe code reviews are key practices to helping all the
team to adopt the same approach and culture.</p>
</blockquote>

<p>Gee actually wrote these words about trunk-based development, yet I find they
apply equally to branch-based development! What happened? Presumptuously, I
suspect Gee spent time with branch-based teams that didn’t observe this culture
of commit hygiene and time with trunk-based teams that did. That kind of
anecdata can taint our view of a workflow <em>even when most of the salient
problems are workflow-independent</em> by <a href="#strawman">creating strawmen out of poor
habits</a>.</p>

<p>I leave you with the following thoughts:</p>
<ul>
  <li>After analysis, the major tradeoffs of either workflow are needed tooling,
conflict resolution, and how to do code review. Most of the rest is a problem
of software development and team culture than a problem of specific choice of
workflow. So <em>pick what works for you</em>.</li>
  <li>Perhaps the true question we should be asking is: what are the tradeoffs of
following or not following good commit hygiene? Most engineers have some
intuition for these tradeoffs, yet we all find large variety in adherence to
commit hygiene. My analysis in this article suggests that good commit hygiene
undergirds most of the advantages Gee ascribes to trunk-based development.
Maybe that’s the real change we need to convince people to make?</li>
  <li>It <em>may</em> be the case that trunk-based development acts as a stronger forcing
function for well-tested code, commit hygiene, and all the rest. Gee makes
some claims to this effect, and I cannot personally evaluate them. I have seen
struggling branch-based teams who adopt better test and commit hygiene come to
benefit—the poster children of good branch-based development would be Git,
Vim, Rust, and similar projects. Which style is more likely to force what
practices is a very different article and set of claims, though!<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></li>
</ul>

<p><a id="postscript"></a>PS Gee later writes in comments:</p>

<blockquote>
  <p>It all comes down to discipline, and the team. For me, I prefer to see
IMMEDIATELY if there are any problems caused by any developer’s commit. I
prefer people not to commit if the trunk is red. I prefer people to fix
problems AS SOON as they occur, and if they cannot be fixed inside 5 minutes
(say), back out that commit and fix it locally. I prefer to work on the same
codebase as everyone else, and not some isolated branch, even if that branch
is only a day old.</p>
</blockquote>

<p>This is what I expected coming in: “my preference is X, but it all comes down to
good hygiene.” I would still have preferred to see what “back out the commit”
means, though I assume it’s some version of <code class="language-plaintext highlighter-rouge">git revert</code>. Even here, “as soon as
they occur” can still be “when PR tests catch the problem.”</p>

<p>Another commenter writes “even if you only ever work on main you absolutely do
have a local branch that is separate from the remote branch,” which is an
insightful perspective: with Git’s model, even centralized workflows don’t force
you to synchronize for every change. There is always more than one branch if you
have a remote.</p>

<p>PPS A future post will cover my own workflows in various setups and
situations—keep an eye on the RSS feed for that!</p>

<h2 id="notes">Notes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:3" role="doc-endnote">
      <p>See <a href="#postscript">the postscript</a> for a shift: in the comments, we see
more of Gee’s thought process. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Though evidence is certainly welcome to inform tradeoffs. See for example
<a href="/blog/2024/09/14/benchmarking-pict-equality-pt-2/">performance comparisons</a> or <a href="https://www.hillelwayne.com/talks/ese/">What we know we
don’t know</a>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>See also <a href="https://locusmag.com/2021/05/cory-doctorow-qualia/">Cory Doctorow’s essay on Qualia</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>Rejected branches that need follow up work to become ready are treated as
new versions even when they share commits; in other words, think
email-driven “v1/2/3” workflows even when working with GitHub’s UI and
rebases. On GitHub, you might also close a PR and start a new one if the new
work is significantly different from the old, whether you rebase or not. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Evaluating them has to be sensitive to seniority, too: if you only have
senior engineers in your trunk pool and juniors in your branch pool, well,
that’s the correlation you measure. And I suspect that many of us that are
in poor branch-based communities now are surrounded by juniors in corporate
environments that don’t reward well the teams who spend time on these
considerations; when we move to a pod of seniors that have already adopted
the baseline assumptions (commit hygiene, etc.), whether they choose
trunk-based or branch-based development, the model surely shines. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="git" /><summary type="html"><![CDATA[If I believe simple questions have complex answers, then it’s no surprise that I have a complicated take on the following question: branch-based or trunk-based development? Which is better, and why?]]></summary></entry><entry><title type="html">Copying a git-range-diff to GitHub</title><link href="https://benknoble.github.io/blog/2024/10/04/copy-range-diff/" rel="alternate" type="text/html" title="Copying a git-range-diff to GitHub" /><published>2024-10-04T00:00:00+00:00</published><updated>2024-10-04T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/10/04/copy-range-diff</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/10/04/copy-range-diff/"><![CDATA[<p>I’ve been using <code class="language-plaintext highlighter-rouge">git range-diff</code> for the past few months to explain changes
between versions of a patch series, such as different versions of a branch after
responding to review comments on a Pull Request. This post explains how I use
post the output for Markdown-ish consumption on GitHub.</p>

<h2 id="primer">Primer</h2>

<p>If you didn’t know, <code class="language-plaintext highlighter-rouge">git range-diff</code> is the standard way in Git to document
changes between versions of a patch series such as you might find sent to a
development mailing list. For example, <code class="language-plaintext highlighter-rouge">git format-patch</code> can include it
automatically in the email so that, when responding to review comments with a
new version, reviewers understand what’s changed.</p>

<p>This all seems only relevant to email-driven workflows, but I argue that it is
also useful for GitHub- or other web- driven workflows. For example: I work on a
branch and submit a Pull Request on GitHub. After some review comments, I may
create some <code class="language-plaintext highlighter-rouge">--fixup</code> commits and <code class="language-plaintext highlighter-rouge">rebase --autosquash</code> them in, perhaps editing
commit messages, or make any number of other changes. When the time comes to
<code class="language-plaintext highlighter-rouge">push --force-with-lease --force-if-includes</code>, the only recourse my reviewers
have to understand the changes is GitHub’s “View changes” button, which attempts
a textual diff between the files at the old and new branch tip.</p>

<p>Yet a range-diff can capture so much more! Consider, for example, this
range-diff from a Racket PR I submitted:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1:  907d3ea366 = 1:  35c19f1e83 docs: capitalize the noun Git
2:  665e037505 ! 2:  6088cd1567 docs: mention the Vi command to add sections
    @@ pkgs/racket-doc/scribblings/style/textual.scrbl: read code on monitors that acco
     +So, when you create a file, add a line with @litchar{;; } followed by ctrl-U 99
     +and @litchar{-}. @margin-note*{In Vi, the command is 99a- followed by Esc.} When
     +you separate "sections" of code in a file, insert the same line. These lines
    -+help both writers and readers to orient themselves in a file. In scribble use
    ++help both writers and readers to orient themselves in a file. In Scribble use
     +@litchar|{@; }| as the prefix.

      @; -----------------------------------------------------------------------------
3:  808676897e = 3:  1e7b35da0a docs: link fx+ and unsafe-fx+
4:  ca7d2a2a56 = 4:  c3e32a5afa docs: correct Git pull command
5:  1108c95343 = 5:  372bbd4ad5 docs: unquote "merge commit"
6:  1374b3b095 &lt; -:  ---------- docs: italicize "e.g."
7:  8f3f1cd517 = 6:  e48525eeb7 docs: correct macro body
-:  ---------- &gt; 7:  38b3c0a75e docs: make explicit the convention for Latin
</code></pre></div></div>

<p>GitHub won’t show you this difference: I capitalized a word in an old commit
message, removed the commit that italicized Latin abbreviations and added one
that clarified said convention. I prefer to provide my reviewers with this
information to help them understand the changes I’ve made (and to help future
readers who may be curious, though I admit this is unlikely in most cases).</p>

<h2 id="sharing-a-range-diff-in-markdown-format">Sharing a range-diff in Markdown format</h2>

<p>A range-diff can get quite large if there are substantial code
changes—arguably, the patch series should become a new branch/PR at such a
point, but that is not often how things operate in practice. I used to paste
range-diffs in code blocks like you see above, but with length and GitHub’s
comment/review interface they became unwieldy.</p>

<p>Instead, I’ve started pasting them inside an HTML <code class="language-plaintext highlighter-rouge">details</code> block so that they
may be collapsed, summarized, and expanded as desired. I often did this by hand,
but <a href="https://github.com/benknoble/Dotfiles/blob/master/links/bin/copy-range-diff">here’s the script I now use called <code class="language-plaintext highlighter-rouge">copy-range-diff</code></a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#! /bin/sh

{
  printf '%s\n' '&lt;details&gt;&lt;summary&gt;range-diff:&lt;/summary&gt;' '' '```'
  cat
  printf '%s\n' '```' '' '&lt;/details&gt;'
} | pbcopy
</code></pre></div></div>

<p>This script reads standard in and pipes a modified version of it to a clipboard
command (Linux users probably prefer an <code class="language-plaintext highlighter-rouge">xsel</code> variant). Placed on the clipboard
after <code class="language-plaintext highlighter-rouge">git range-diff … | copy-range-diff</code> is an HTML <code class="language-plaintext highlighter-rouge">details</code> block containing
a Markdown code-fence which is easy to paste into GitHub or similar interfaces.
Sometimes I will add a short summary to the summary tag; other times, I leave
just the mention of a range-diff.</p>

<p>A small tweak should work for pure HTML output so that instead of triple-tick
Markdown fences we emit <code class="language-plaintext highlighter-rouge">&lt;pre&gt;</code> tags.</p>

<details><summary>Here's the earlier range-diff, in a details block</summary>

<!-- For some reason, Jekyll doesn't know how to process Markdown fences here,
     so trick it with HTML. -->

<pre class="highlight">
<code>1:  907d3ea366 = 1:  35c19f1e83 docs: capitalize the noun Git
2:  665e037505 ! 2:  6088cd1567 docs: mention the Vi command to add sections
    @@ pkgs/racket-doc/scribblings/style/textual.scrbl: read code on monitors that acco
     +So, when you create a file, add a line with @litchar{;; } followed by ctrl-U 99
     +and @litchar{-}. @margin-note*{In Vi, the command is 99a- followed by Esc.} When
     +you separate "sections" of code in a file, insert the same line. These lines
    -+help both writers and readers to orient themselves in a file. In scribble use
    ++help both writers and readers to orient themselves in a file. In Scribble use
     +@litchar|{@; }| as the prefix.

      @; -----------------------------------------------------------------------------
3:  808676897e = 3:  1e7b35da0a docs: link fx+ and unsafe-fx+
4:  ca7d2a2a56 = 4:  c3e32a5afa docs: correct Git pull command
5:  1108c95343 = 5:  372bbd4ad5 docs: unquote "merge commit"
6:  1374b3b095 &lt; -:  ---------- docs: italicize "e.g."
7:  8f3f1cd517 = 6:  e48525eeb7 docs: correct macro body
-:  ---------- &gt; 7:  38b3c0a75e docs: make explicit the convention for Latin</code>
</pre>

</details>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="git" /><summary type="html"><![CDATA[I’ve been using git range-diff for the past few months to explain changes between versions of a patch series, such as different versions of a branch after responding to review comments on a Pull Request. This post explains how I use post the output for Markdown-ish consumption on GitHub.]]></summary></entry><entry><title type="html">Performance of Racket Pict Comparison, Part 2</title><link href="https://benknoble.github.io/blog/2024/09/14/benchmarking-pict-equality-pt-2/" rel="alternate" type="text/html" title="Performance of Racket Pict Comparison, Part 2" /><published>2024-09-14T00:00:00+00:00</published><updated>2024-09-14T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/09/14/benchmarking-pict-equality-pt-2</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/09/14/benchmarking-pict-equality-pt-2/"><![CDATA[<p>I eliminate “eyeball statistics” from <a href="/blog/2024/02/15/benchmarking-pict-equality/">part 1</a>. This post is based on
<a href="https://chelseatroy.com/2021/02/26/data-safety-for-programmers/">Chelsea Troy’s “Data Safety”
series</a>,
especially <a href="https://chelseatroy.com/2021/03/12/quantitative-programming-knife-skills-part-2/">Quantitative Programming Knife Skills, Part
2</a>.</p>

<p>As usual, all code is <a href="https://github.com/benknoble/pict-equal-bench">available on GitHub</a>.</p>

<h2 id="problems">Problems</h2>

<p>In the previous post, I eyeballed distributions from box-and-whisker plots (in
addition to relying on reported timings from hyperfine) to determine which
method of comparing <code class="language-plaintext highlighter-rouge">pict</code>s is faster. Today, we’ll look at addressing two
limitations of that approach:</p>

<ol>
  <li>How confident are we that the true mean of relevant measurements is captured
by the mean of our sample distribution? We’ll compute confidence intervals
for an appropriate distribution.</li>
  <li>How likely is it that the distributions actually differ for a reason other
than random chance? We’ll use statistical tests to measure the probability of
difference being attributable to random chance (the “null hypothesis”).</li>
</ol>

<p>First, though, we’ve got to talk about distributions.</p>

<h2 id="distributions">Distributions</h2>

<p>We have (by computation) a mean and standard deviation for various facets of our
<a href="https://github.com/benknoble/pict-equal-bench">data</a>. We have enough data that assuming a normal distribution is not
unreasonable; let’s take a look. For the time benchmark data, the following code</p>

<div class="language-racket highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">require</span> <span class="nv">sawzall</span>
         <span class="nv">data-frame</span>
         <span class="nv">threading</span>
         <span class="nv">math/statistics</span><span class="p">)</span>

<span class="p">(</span><span class="k">define</span> <span class="p">(</span><span class="nf">v-μ</span> <span class="nv">xs</span><span class="p">)</span>
  <span class="p">(</span><span class="nb">exact-&gt;inexact</span> <span class="p">(</span><span class="nf">mean</span> <span class="p">(</span><span class="nb">vector-&gt;list</span> <span class="nv">xs</span><span class="p">))))</span>

<span class="p">(</span><span class="k">define</span> <span class="p">(</span><span class="nf">v-σ</span> <span class="nv">xs</span><span class="p">)</span>
  <span class="p">(</span><span class="nf">stddev</span> <span class="p">(</span><span class="nb">vector-&gt;list</span> <span class="nv">xs</span><span class="p">)))</span>

<span class="p">(</span><span class="k">define</span> <span class="nv">t-short</span> <span class="p">(</span><span class="nf">df-read/csv</span> <span class="s">"time.csv"</span><span class="p">))</span>

<span class="p">(</span><span class="nf">~&gt;</span> <span class="nv">t-short</span>
    <span class="p">(</span><span class="nf">group-with</span> <span class="s">"bench"</span><span class="p">)</span>
    <span class="p">(</span><span class="nf">aggregate</span> <span class="p">[</span><span class="nf">cpu-μ</span> <span class="p">(</span><span class="nf">cpu</span><span class="p">)</span> <span class="p">(</span><span class="nf">v-μ</span> <span class="nv">cpu</span><span class="p">)]</span>
               <span class="p">[</span><span class="nf">cpu-σ</span> <span class="p">(</span><span class="nf">cpu</span><span class="p">)</span> <span class="p">(</span><span class="nf">v-σ</span> <span class="nv">cpu</span><span class="p">)]</span>
               <span class="p">[</span><span class="nf">real-μ</span> <span class="p">(</span><span class="nf">real</span><span class="p">)</span> <span class="p">(</span><span class="nf">v-μ</span> <span class="nv">real</span><span class="p">)]</span>
               <span class="p">[</span><span class="nf">real-σ</span> <span class="p">(</span><span class="nf">real</span><span class="p">)</span> <span class="p">(</span><span class="nf">v-σ</span> <span class="nv">real</span><span class="p">)]</span>
               <span class="p">[</span><span class="nf">gc-μ</span> <span class="p">(</span><span class="nf">gc</span><span class="p">)</span> <span class="p">(</span><span class="nf">v-μ</span> <span class="nv">gc</span><span class="p">)]</span>
               <span class="p">[</span><span class="nf">gc-σ</span> <span class="p">(</span><span class="nf">gc</span><span class="p">)</span> <span class="p">(</span><span class="nf">v-σ</span> <span class="nv">gc</span><span class="p">)])</span>
    <span class="p">(</span><span class="nf">show</span> <span class="nv">everything</span><span class="p">))</span>
</code></pre></div></div>

<p>produces</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data-frame: 2 rows x 7 columns
┌──────────────────┬─────────┬──────────────────┬──────────────────┬─────────────────┬───────────────────┬────────────────────┐
│cpu-σ             │bench    │real-μ            │real-σ            │cpu-μ            │gc-σ               │gc-μ                │
├──────────────────┼─────────┼──────────────────┼──────────────────┼─────────────────┼───────────────────┼────────────────────┤
│1.477002129744432 │bytes    │4.225773195876289 │1.4978411739022484│4.175601374570447│0.15933274204104092│0.006529209621993127│
├──────────────────┼─────────┼──────────────────┼──────────────────┼─────────────────┼───────────────────┼────────────────────┤
│1.1370657205199777│record-dc│2.0853951890034366│1.148475651239016 │2.05893470790378 │0.16564821501258256│0.007216494845360825│
└──────────────────┴─────────┴──────────────────┴──────────────────┴─────────────────┴───────────────────┴────────────────────┘
</code></pre></div></div>

<p>Let’s plot these (<a href="https://github.com/benknoble/pict-equal-bench">code</a>):</p>

<p><img src="/assets/img/pict-time-bench-cpu-normal.svg" alt="CPU time normal distributions" />
<img src="/assets/img/pict-time-bench-real-normal.svg" alt="Real time normal distributions" />
<img src="/assets/img/pict-time-bench-gc-normal.svg" alt="GC time normal distributions" /></p>

<p>By now you might have realized that negative times don’t make sense… this
suggests the normal distribution is not an appropriate distribution for
comparison. <a href="https://stats.stackexchange.com/a/203958">It appears that the exponential or Weibull distributions might
model this process better</a>, but for
now we’ll continue taking timings and memory usage to be normally distributed
for simplification.</p>

<p>Here are the equivalent values and plots for memory (all in MiB):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data-frame: 2 rows x 3 columns
┌───────────────────┬──────────────────┬─────────┐
│memory-σ           │memory-μ          │bench    │
├───────────────────┼──────────────────┼─────────┤
│0.22898033579570978│0.6772037663410619│bytes    │
├───────────────────┼──────────────────┼─────────┤
│0.23704268897914643│0.5486571085821722│record-dc│
└───────────────────┴──────────────────┴─────────┘
</code></pre></div></div>

<p><img src="/assets/img/pict-time-bench-memory-normal.svg" alt="Memory normal distributions in MiB" /></p>

<h2 id="confidence-intervals">Confidence Intervals</h2>

<p>To compute a confidence interval, we’ll take the distributions to be
t-distributed and compute properties of the t-statistic (for \(\alpha = 0.05\),
a 95% confidence interval). This interval tells us where the true mean falls
with 95% probability.</p>

<p>Let’s use a critical value of 1.96, which corresponds to an assumption that the
distributions are normal (given our large sample size, the t-distribution is
close to normal) and a 95% confidence interval. The formula (letting \(s\) be
the sample standard deviation, \(\bar{x}\) the sample mean, and \(N\) the number
of samples) is</p>

\[\bar{x} \pm 1.96 \frac{s}{N}\]

<p>Here are the low and high offsets of the intervals for memory and time:</p>

<ul>
  <li>memory
    <ul>
      <li>bytes: [0.6771 MiB, 0.6773 MiB]</li>
      <li>record-dc: [0.5486 MiB, 0.5487 MiB]</li>
    </ul>

    <p><img src="/assets/img/pict-time-bench-memory-confidence.svg" alt="Memory confidence intervals in MiB" /></p>
  </li>
  <li>cpu
    <ul>
      <li>bytes: [4.175, 4.176]</li>
      <li>record-dc: [2.0586, 2.0593]</li>
    </ul>

    <p><img src="/assets/img/pict-time-bench-cpu-confidence.svg" alt="CPU time confidence intervals" /></p>
  </li>
  <li>real
    <ul>
      <li>bytes: [4.225, 4.226]</li>
      <li>record-dc: [2.085, 2.086]</li>
    </ul>

    <p><img src="/assets/img/pict-time-bench-real-confidence.svg" alt="Real time confidence intervals" /></p>
  </li>
  <li>gc
    <ul>
      <li>bytes: [0.00648, 0.00658]</li>
      <li>record-dc: [0.00716, 0.00727]</li>
    </ul>

    <p><img src="/assets/img/pict-time-bench-gc-confidence.svg" alt="GC time confidence intervals" /></p>
  </li>
</ul>

<p>The plots are zoomed in because the intervals are so narrow thanks to our high
number of samples. We can be very confident that our sample means are close to
the true mean.</p>

<h2 id="statistical-tests">Statistical tests</h2>

<p>We want to compare the memory distributions across both benchmarks, and each of
the real, cpu, and gc times across both benchmarks. We’ll use the
<a href="https://docs.racket-lang.org/t-test/index.html"><code class="language-plaintext highlighter-rouge">welch-t-test</code> function from the <code class="language-plaintext highlighter-rouge">t-test</code>
library</a><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, since we can’t assume
the variances are equal (though eyeball statistics suggests that gc variance is,
which is sensible since most gc times are 0).</p>

<p>The code to compute a t-test for memory distributions is short:</p>
<div class="language-racket highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">require</span> <span class="nv">threading</span>
         <span class="nv">data-frame</span>
         <span class="nv">sawzall</span>
         <span class="nv">t-test</span><span class="p">)</span>

<span class="p">(</span><span class="k">define</span> <span class="nv">m</span>
  <span class="p">(</span><span class="nf">~&gt;</span> <span class="s">"memory.csv"</span> <span class="nv">df-read/csv</span>
      <span class="p">(</span><span class="nf">create</span> <span class="p">[</span><span class="nf">memory</span> <span class="p">(</span><span class="nf">memory</span><span class="p">)</span> <span class="p">(</span><span class="nb">/</span> <span class="nv">memory</span> <span class="p">(</span><span class="nb">expt</span> <span class="mi">2</span> <span class="mi">20</span><span class="p">))])))</span>

<span class="p">(</span><span class="nb">apply</span>
 <span class="nv">welch-t-test</span>
 <span class="p">(</span><span class="nf">~&gt;</span> <span class="nv">m</span>
     <span class="p">(</span><span class="nf">split-with</span> <span class="s">"bench"</span><span class="p">)</span>
     <span class="p">(</span><span class="nb">map</span> <span class="p">(</span><span class="k">λ</span> <span class="p">(</span><span class="nf">df</span><span class="p">)</span>
            <span class="p">(</span><span class="nf">~&gt;</span> <span class="nv">df</span>
                <span class="p">(</span><span class="nf">slice</span> <span class="p">[</span><span class="nf">"memory"</span><span class="p">])</span>
                <span class="p">(</span><span class="nf">df-select</span> <span class="s">"memory"</span><span class="p">)))</span>
          <span class="nv">_</span><span class="p">)))</span>
</code></pre></div></div>

<p>The result for a p-value of \(0.01\) is \(1.53 \times 10^{-187}\). That is, we
can be extremely confident that the distributions have different means: we
reject the null hypothesis that the distributions have the same mean as the
likelihood of this sample occurring given said hypothesis is nearly 0.</p>

<p>Of note, this test suggests the distributions are different <em>despite
having close means</em>: the difference between the means (0.12854665775888974) is
23.42% of the smaller of the two means and 18.98% of the larger.</p>

<p>The code for time distributions is parameterized on the column:</p>

<div class="language-racket highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">require</span> <span class="nv">threading</span>
         <span class="nv">data-frame</span>
         <span class="nv">sawzall</span>
         <span class="nv">t-test</span><span class="p">)</span>

<span class="p">(</span><span class="k">define</span> <span class="nv">t</span> <span class="p">(</span><span class="nf">df-read/csv</span> <span class="s">"time.csv"</span><span class="p">))</span>

<span class="p">(</span><span class="nf">for/list</span> <span class="p">([</span><span class="nf">column</span> <span class="o">'</span><span class="p">(</span><span class="nf">"cpu"</span> <span class="s">"real"</span> <span class="s">"gc"</span><span class="p">)])</span>
  <span class="p">(</span><span class="nb">list</span> <span class="nv">column</span>
        <span class="p">(</span><span class="nb">apply</span>
         <span class="nv">welch-t-test</span>
         <span class="p">(</span><span class="nf">~&gt;</span> <span class="nv">t</span>
             <span class="p">(</span><span class="nf">slice</span> <span class="p">(</span><span class="nf">all-in</span> <span class="p">(</span><span class="nb">list</span> <span class="s">"bench"</span> <span class="nv">column</span><span class="p">)))</span>
             <span class="p">(</span><span class="nf">split-with</span> <span class="s">"bench"</span><span class="p">)</span>
             <span class="p">(</span><span class="nb">map</span> <span class="p">(</span><span class="k">λ</span> <span class="p">(</span><span class="nf">df</span><span class="p">)</span>
                    <span class="p">(</span><span class="nf">~&gt;</span> <span class="nv">df</span>
                        <span class="p">(</span><span class="nf">slice</span> <span class="nv">column</span><span class="p">)</span>
                        <span class="p">(</span><span class="nf">df-select</span> <span class="nv">column</span><span class="p">)))</span>
                  <span class="nv">_</span><span class="p">)))))</span>
</code></pre></div></div>

<p>The results (again with p-value \(0.01\)):</p>
<ul>
  <li>cpu: \(0.0\)</li>
  <li>real: \(0.0\)</li>
  <li>gc: \(0.8196\)</li>
</ul>

<p>I’m actually shocked that the procedure produced an (inexact) 0, and I checked
the data being fed in. I can’t explain the result beyond gesturing at floating
point math; there’s no statistical realm where we accept that this occurrence is
impossible. For now, I’ll content myself with supposing that the calculation
produces such a small number that even Racket can’t keep up, and reject the null
hypothesis for CPU and real times (there is a statistically meaningful
difference in the time the two procedures take).</p>

<p>For GC times, of course, there is insufficient evidence to reject the null
hypothesis <em>as expected</em>. Most GC times are 0! It’s reasonably more likely that
the underlying distributions are actually the same one.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I feel justified in my choice of method based on the data from last time and
satisfied that I’m not relying entirely on eyeball statistics. The lack of
explanation for \(0.0\) is dissatisfying…</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>There have been <a href="https://racket.discourse.group/t/could-the-t-student-distribution-be-included-in-the-math-module/2999">several interesting conversations about performing
t-tests within Racket
lately</a>,
which lead to me include these additional links: <a href="https://onecompiler.com/racket/42j3n4wdn">pasted
code</a>, <a href="https://github.com/soegaard/math/blob/student-t/math-lib/math/private/distributions/impl/student-t.rkt">@soegaard’s
fork</a>
<a href="https://math.stackexchange.com/questions/4367570/transform-students-t-distribution-to-beta-distribution">how to build the distribution from its
parts</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="racket" /><category term="performance" /><category term="statistics" /><summary type="html"><![CDATA[I eliminate “eyeball statistics” from part 1. This post is based on Chelsea Troy’s “Data Safety” series, especially Quantitative Programming Knife Skills, Part 2.]]></summary></entry><entry><title type="html">Churn and Weight</title><link href="https://benknoble.github.io/blog/2024/08/07/churn-and-weight/" rel="alternate" type="text/html" title="Churn and Weight" /><published>2024-08-07T00:00:00+00:00</published><updated>2024-08-07T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/08/07/churn-and-weight</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/08/07/churn-and-weight/"><![CDATA[<p>I examine a metric that might be correlated with churn, and I posit a new
concept for code bases called weight. I also call attention to 2 tools for
measuring churn and weight.</p>

<h2 id="churn">Churn</h2>

<p>Churn is an attribute of functions and modules: those which tend to change
frequently are said to undergo heavy churn. This is typically a symptom of tight
coupling.</p>

<p>Measuring churn can be done, for example, by looking at how often files change
in Git history. In 2018 I stole <a href="https://github.com/garybernhardt/dotfiles/blob/main/bin/git-churn">Gary Bernhardt’s
<code class="language-plaintext highlighter-rouge">git-churn</code></a>
and <a href="https://github.com/benknoble/Dotfiles/blob/master/links/bin/git-churn">made it my
own</a>. For
example, here’s what it says on Frosthaven Manager:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># git churn | head</span>
 117 scribblings/reference.scrbl
 101 server.rkt
  94 gui/manager.rkt
  90 gui/monsters.rkt
  77 defns.rkt
  73 info.rkt
  66 monster-db.rkt
  49 manager/state.rkt
  44 manager.rkt
  35 elements.rkt
</code></pre></div></div>

<p>Unsurprisingly, the reference documentation and web server change the most
frequently, followed closely by 2 of the largest and most important GUI
components (the composite whole and the monsters). The reference documentation
changes whenever a module is added, moved, or renamed, and the web server is
changing rapidly in response to player feedback.</p>

<p>Let’s examine a related idea: <em>weight</em>.</p>

<h2 id="weight">Weight</h2>

<p>I’m using the word “weight” to refer to both how heavy a module or function is
in terms of essential complexity and to how load-bearing it is in terms of
making the system do what it does. In a mature project, we probably expect to
find a few core modules to be heavy, with a (possibly long) tail of light
supplements.</p>

<p>Since heavy modules often sit at the core of the system, they are likely to have
either lots of churn (frequent changes to the core system) or little (a stable
core with frequent changes to the supplements). In other words, churn on heavy
core modules can give us an idea of how stable our core is: having the wrong
core design is a serious challenge to the life of the project.</p>

<p>How do we measure weight? As a start, I’ll use <em>relative percentages of the
code.</em> This sounds like lines of code, but it has a subtle difference: 10k lines
of code is only 1% of a codebase with 1 million lines, but dwarfs any system
with a mere few hundred. It’s not size that matters, it’s relative size.</p>

<p>Using my
<a href="https://github.com/benknoble/Dotfiles/blob/master/links/bin/code-percent"><code class="language-plaintext highlighter-rouge">code-percent</code></a>
program, we can tabulate relevant percentages and their cumulative effects.
Here’s how it runs on Frosthaven Manager:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># git ls-files | code-percent</span>
RowNumber  File                                      PercentTotal  CumulativePercentTotal
1          server.rkt                                7.47149       7.47149
2          gui/monsters.rkt                          6.37762       13.8491
3          scribblings/programming-scenario.scrbl    4.56775       18.4169
4          manager/state.rkt                         3.9048        22.3217
5          defns/monsters.rkt                        3.62636       25.948
6          gui/manager.rkt                           2.87059       28.8186
7          gui/player-info.rkt                       2.27393       31.0925
8          gui/markdown.rkt                          2.11482       33.2074
9          elements.rkt                              1.93583       35.1432
10         scribblings/how-to-play.scrbl             1.9292        37.0724
<span class="c"># [snip]</span>
162        README.md                                 0.0265182     100
</code></pre></div></div>

<p>This gives a different picture of the code. The reference has disappeared
(supplanted by large documents), being a mere 30 lines relative to the 15k
total. But a few core pieces of functionality (state, game definitions, and GUI
pieces) have drifted to the surface, collectively taking up roughly one third of
the size of the code. Indeed, we might now question if <code class="language-plaintext highlighter-rouge">gui/markdown.rkt</code>, which
implements a “good enough for me” Markdown-to-GUI-text widget, is holding its
weight. Conversely, it might be time to try refactoring the server or monsters
GUI again. The server is not core in the layering of pieces, but it is core to
the app’s experience. It might even contain its own core of domain pieces, which
explains its weight.</p>

<p>One other trick we can do: we can get a neat idea of the size of the tail by
showing only the rows that bump us over major (cumulative) thresholds. For
example:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># git ls-files | code-percent | awk 'BEGIN { target = 25; } NR == 1 { print; next; } $4 &gt;= target { print; target += 25; }'</span>
RowNumber  File                                      PercentTotal  CumulativePercentTotal
5          defns/monsters.rkt                        3,62636       25,948
19         defns/scenario.rkt                        1,18006       50,3116
50         gui/table.rkt                             0,550252      75,4243
162        README.md                                 0,0265182     100
</code></pre></div></div>

<p>While the first 5 files account for 25% of the code, it takes almost another 15
to reach 50%, then another 30 to reach 75% followed by a whopping 110 to
conclude. This matches at least our expectation of a long tail.</p>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="project-management" /><category term="git" /><category term="frosthaven-manager" /><category term="racket" /><summary type="html"><![CDATA[I examine a metric that might be correlated with churn, and I posit a new concept for code bases called weight. I also call attention to 2 tools for measuring churn and weight.]]></summary></entry><entry><title type="html">Developer Experience, Redux</title><link href="https://benknoble.github.io/blog/2024/08/03/devex-redux/" rel="alternate" type="text/html" title="Developer Experience, Redux" /><published>2024-08-03T00:00:00+00:00</published><updated>2024-08-03T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/08/03/devex-redux</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/08/03/devex-redux/"><![CDATA[<p>I pull excerpts from recent <em>Communications of the ACM</em> articles relevant to
developer experience advocates and add my own commentary.</p>

<h2 id="resistance-is-your-friend">Resistance is your friend</h2>

<p>Our first article<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> is an opinion piece on the nature of resistance to change.
Denning and Lyons argue that human communities prefer equilibrium to disruption
and present a framework for using that resistance to build a better case for
your new idea, disruption, or innovation. In particular, they note that only
changes in a communities otherwise-satisfied concerns will open the community to
a new equilibrium:</p>

<blockquote>
  <p>An innovation leader proposing a change of practice needs to understand and
address those concerns. Rather than run away from the resistance or trying to
overcome it by force, leaders move toward the resistance with curiosity and
humility to understand why people are committed to the current equilibrium.
The goal is discover latent concerns, which when brought to their awareness
will motivate people to move toward the proposed change. You will not find a
cause for the resistance by looking at external circumstances. You will find
its causes in the everyday conversations of people in the community.</p>
</blockquote>

<p>They write that their experience demonstrates that the best leaders “bend” with
resistance:</p>

<blockquote>
  <p>Flow with the resistance, seeking to understand the concerns behind it and
revising offers to take care of those concerns. Mobilize followers to build
social power behind your offers and neutralize the social power of the
resistance.</p>
</blockquote>

<p>What does this tell us about developer experience? Developer tools cannot be
imposed by fiat or mandate: you risk a resistance movement that outlives any
particular persons ability to declare “the way.” Worse, you don’t have the
support of the community. There may be a period of perceived success, but it
usually eventually crumbles.</p>

<p>Instead, developer tools are best created by motivating the community, listening
to their problems, and solving them together. This allows you to mobilize
excited early adopters to pave the way for majority adopters.</p>

<h2 id="devex-in-action">DevEx in Action</h2>

<p>Our next article<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> is a study of “developer experience and its tangible
impact.” My one critique is that the article doesn’t seem to consider team
turnover and the possible impacts to developer experience there: experience
suffers when picking up a project whose authors are no longer around regardless
of the state of anything else, but certain practices make this easier than
others.</p>

<p>This article uses surveys and statistical techniques to examine a proposed model
of developer experience concerns and relate improvements in developer experience
to tangible individual, team, and organizational outcomes. The statistical
connection provides powerful motivation for business to improve developer
experience:</p>

<blockquote>
  <p>Now that you are sold on improving DevEx, how can you convince your
organization to buy in? First, have them read this article. Then, joke aside,
here are five important steps that can help you advocate for continuous
improvements by keeping your arguments grounded in data.</p>
</blockquote>

<p>The proposed model incorporates flow state, feedback loops, and cognitive load
as the concerns of developer experience. Studied individual factors include
learning, job performance, and creativity. Team factors include code
quality and technical debt. Organizational factors include retention and
profitability. The statistical findings indicate that, for their sample, flow
state and cognitive load positively impacts all outcomes. Feedback loops
influence team outcomes but not individual or organizational outcomes (see the
article for full details).</p>

<p>Here are a few topics I want to highlight.</p>

<h3 id="git">Git</h3>

<p>As many of my <a href="/tags#capital+one" class="tag">Capital One</a> coworkers
know, I have spent a lot of time talking about the many ways in which <a href="/tags#git" class="tag">Git</a> can improve developer experience if only we
spent the time to take advantage of our tools.</p>

<p>Speed and quality of information (“How often it takes &gt;10min to have a question
answered”) is an important aspect of the article’s conception of feedback loops;
this is supported by prior research. I have long argued that Git as a version
control system tracks <em>by default</em> the who, what, when, and where. Arguably the
how can be captured as well when the patches include mechanisms. The only person
who can answer <em>why</em> is the commit author: take the time to write down why the
changes are necessary or important! Then Git becomes the ultimate speedy
answerer: search techniques from code search with <code class="language-plaintext highlighter-rouge">git grep</code> to sophisticated
history searches with <code class="language-plaintext highlighter-rouge">git log</code> and <code class="language-plaintext highlighter-rouge">-G</code>, <code class="language-plaintext highlighter-rouge">-S</code>, or <code class="language-plaintext highlighter-rouge">--grep</code> empower us to find
more information or more threads to tug on from the comfort of our workstation
and development environments. Even <code class="language-plaintext highlighter-rouge">git blame</code> has a role to play in helping us
understand our work. The authors write:</p>

<blockquote>
  <p>Teams that provide fast responses to developers’ questions report 50% less
technical debt than teams whose responses are slow. It pays to document
repeated developer questions and/or put tooling in place so developers can
easily and quickly navigate to the response they need and integrate good
coding practices and solutions as they write their code, which creates less
technical debt.</p>
</blockquote>

<p>Certainly version control is not the only place for such Q&amp;A documentation, but
it should be treated as the wealth of information that it is. This also suggests
that how to find answers to questions is a major (and trainable) skill.</p>

<p>Git shines not only in feedback loops: Git helps offload cognitive burdens, too.
We know that the human brain tends to hold on to incomplete tasks and create
stress while it is able to quickly erase completed tasks and associated stress.
Writing down our thoughts in a commit message is a form of completing the task,
relieving stress. It further enables us to search, not sort, our information:
sorting information is a maintenance intensive burden that doesn’t always lead
to the information being easy to find. Commit messages offload our thoughts into
a kind of searchable exobrain that we share with our teams and our future
colleagues, whether we’re around to work with them or not. Finally, commit
messages assist developers who want to understand code, which is an important
factor in the article’s consideration of cognitive load.</p>

<h3 id="impact">Impact</h3>

<p>I’m not sure much else needs said here:</p>

<blockquote>
  <p>To improve developer outcomes, deep work and engaging work have the biggest
potential impact. To improve organizational outcomes, several items have the
potential for big impact: deep work, engaging work, intuitive processes, and
intuitive developer tools</p>
</blockquote>

<p>By the way, the opposite also holds true:</p>

<blockquote>
  <p>Developers who find their tools and work processes intuitive and easy to use
feel they are 50% more innovative compared with those with opaque or
hard-to-understand processes. Unintuitive tools and processes can be both a
time sink and a source of frustration—in either case, a severe hindrance to
individuals’ and teams’ creativity.</p>
</blockquote>

<p>Since I have otherwise less to say about a third article<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, I’ll quote its
knowledge of automation processes such as CI (see original for citations):</p>

<blockquote>
  <p>However, an immature automation process can result in negative outcomes, such
as cost and schedule overruns, slow feedback loops, and delayed releases.</p>
</blockquote>

<p>“DevEx in Action” presents a framework for making an impact consists of 5 steps,
which I’ll summarize here:</p>

<blockquote>
  <ol>
    <li>Get data on the current developer experience.</li>
    <li>Set goals based on your DevEx data.</li>
    <li>Set your team up for success.</li>
    <li>Share progress and validate investments.</li>
    <li>Repeat the process.</li>
  </ol>
</blockquote>

<p>Coupled with the previous article’s direction on resistance, this provides a
powerful roadmap from which to iterate and improve outcomes.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://cacm.acm.org/opinion/resistance-is-your-friend/">Peter Denning and Todd Lyons. 2024. Resistance Is Your Friend. <em>Commun.
ACM 67</em>, 6 (June 2024), 26–29.
https://doi.org/10.1145/3654696</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://cacm.acm.org/practice/devex-in-action/">Nicole Forsgren, Eirini Kalliamvakou, Abi Noda, Michaela Greiler, Brian
Houck, and Margaret-Anne Storey. 2024. DevEx in Action. <em>Commun. ACM 67</em>, 6
(June 2024), 42–51.
https://doi.org/10.1145/3643140</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://cacm.acm.org/research/a-roadmap-for-using-continuous-integration-environments/">Liang Yu, Emil Alégroth, Panagiota Chatzipetrou, and Tony Gorschek. 2024.
A Roadmap for Using Continuous Integration Environments. Commun. ACM 67, 6
(June 2024), 82–90.
https://doi.org/10.1145/3631519</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="developer experience" /><category term="acm" /><summary type="html"><![CDATA[I pull excerpts from recent Communications of the ACM articles relevant to developer experience advocates and add my own commentary.]]></summary></entry><entry><title type="html">What everyone should know about git commit</title><link href="https://benknoble.github.io/blog/2024/08/02/git-commit/" rel="alternate" type="text/html" title="What everyone should know about git commit" /><published>2024-08-02T00:00:00+00:00</published><updated>2024-08-02T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/08/02/git-commit</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/08/02/git-commit/"><![CDATA[<p>These tips make it easier to write better commit messages.</p>

<ol>
  <li>Stop using <code class="language-plaintext highlighter-rouge">-m</code> (configure a proper editor)</li>
  <li>Write while looking at the diff with <code class="language-plaintext highlighter-rouge">-v</code></li>
</ol>

<h2 id="stop-using-git-commit--m">Stop using <code class="language-plaintext highlighter-rouge">git commit -m</code></h2>

<p>Using <code class="language-plaintext highlighter-rouge">-m</code> encourages short commit messages that don’t give meaningful
information. Did you know that omitting <code class="language-plaintext highlighter-rouge">-m</code> gives you a chance to write your
commit in your favorite editor? I think many people assume that <code class="language-plaintext highlighter-rouge">-m</code> is the only
way to avoid being dropped in to Vi (oh, the horror… <a href="/about/">I guess</a>…), but you can <a href="https://git-scm.com/book/en/v2/Appendix-C%3A-Git-Commands-Setup-and-Config">make Git use any editor you
want</a>.
Now you have room to write those <a href="https://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html">standard formatted commit
messages</a>.</p>

<p>The <code class="language-plaintext highlighter-rouge">-m</code> flag is convenient for <code class="language-plaintext highlighter-rouge">wip</code>-style save-my-progress commits that you’ll
rebase and reword. You will reword them, won’t you?</p>

<h2 id="use--v-to-see-your-changes-in-the-commit-message-template">Use <code class="language-plaintext highlighter-rouge">-v</code> to see your changes in the commit message template</h2>

<p>You’re writing your commit message and you want to review the changes to make
sure you commented on everything. What do you do?</p>

<ul>
  <li>I run <a href="https://github.com/benknoble/Dotfiles/blob/2ba059a73eb38b96225cc770f8e7d4d05b970306/links/vim/after/ftplugin/gitcommit.vim#L16"><code class="language-plaintext highlighter-rouge">:DiffGitCached</code> with
<code class="language-plaintext highlighter-rouge">\v</code></a>,
personally. Or <code class="language-plaintext highlighter-rouge">:Git --paginate diff --cached</code>.</li>
  <li>Use your terminal to do <code class="language-plaintext highlighter-rouge">git diff --cached</code>.</li>
  <li>Scroll to the bottom, because you used <code class="language-plaintext highlighter-rouge">--verbose</code>.</li>
</ul>

<p>See that last one? If you need to, turn on <code class="language-plaintext highlighter-rouge">commit.verbose</code>; then Git will
include the diff in your commit message template for you to refer to.</p>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="git" /><summary type="html"><![CDATA[These tips make it easier to write better commit messages.]]></summary></entry><entry><title type="html">The case against squashing to merge via GitHub</title><link href="https://benknoble.github.io/blog/2024/08/02/github-squash/" rel="alternate" type="text/html" title="The case against squashing to merge via GitHub" /><published>2024-08-02T00:00:00+00:00</published><updated>2024-08-02T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/08/02/github-squash</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/08/02/github-squash/"><![CDATA[<p>Squash and merge is a bad default action, and it grates when it’s the <em>only</em>
permitted merge option.</p>

<p>Squash and merge collapses even carefully crafted PRs and commits into a giant
blob (pun intended). All the juicy information we know how to unlock with tools
like <code class="language-plaintext highlighter-rouge">git log</code> and <code class="language-plaintext highlighter-rouge">git blame</code> is now gone, made useless by the squash. It even
hurts <code class="language-plaintext highlighter-rouge">git bisect</code>: pinpointing a squashed merge is unlikely to help if you
still have to wade through 6000 lines of changes instead of 600 or 60. Even if
you can recover the original branch using GitHub’s <code class="language-plaintext highlighter-rouge">refs/pull/N/head</code> namespace,
which isn’t guaranteed on other vendors, it can still be harmful to hit the
squash fence.</p>

<p>Worse, squashing to merge traps you in a vicious cycle. In an ideal world, good
commits help us fall into a virtuous cycle of success: they provide more
information so that our tools are more useful, which encourages us to use our
tools more, which reveals more opportunities to pack information into commits,
and so on. Meanwhile, squashing everything by default traps us in the antithesis
of this cycle—commit messages are useless if they’ll be squashed, so they become
useless in a self-fulfilling prophecy, encouraging squash and merge ad nauseum.
We remove incentives to use our information-gathering tools; not using them
blinds us to their power, keeping them from being useful.</p>

<p>No other investment that we make pays off as long as good commits. Good commits
pay dividends for the entire lifetime of a project and become more valuable the
longer the project lives. This might as well be an exponential curve of payoff.
Nothing is free, but the expected value of any single commit dwarfs the cost of
the minutes involved in crafting its message. Squash and merge as a default
eliminates those benefits and any incentives to reap them. Outside of a few
narrow use cases, I recommend against it.</p>

<p>Discuss with your teams and projects: decide what’s right for you. Don’t default
to squash and merge because it seems easy or commonplace. One of the trade-offs
is giving up useful commits (unless whoever merges takes the time to craft a
good squash message, which in many circles is rare thanks to GitHub defaults).
If you intentionally choose this trade-off, you probably needing to be getting
equivalent dividends from whatever the other trade-offs are. The benefits of
commits are hard to measure up against.</p>

<p>One case squash might be useful: squashing messy external contributions gives
maintainers an expedient way to accept otherwise good code and bring the commit
up to project standards. Ideally this is paired with resources for the
contributor to learn how to do better next time, including both Git knowledge
and project standards knowledge. Mentoring a new contributor through the process
to fix the commits themselves is a valuable and laudable task, but it is costly.</p>

<p>On the other hand, moving slowly is not so bad as it seems, despite all FAANG
would have you believe.</p>

<p>Don’t forget: GitHub foisted the squash and merge option upon us with bad
defaults to back it up. To do the equivalent in Git requires knowledge of
interactive rebase or the <code class="language-plaintext highlighter-rouge">cherry-pick</code> command. Git knows how to merge and
rebase via commands of the same name and flags to <code class="language-plaintext highlighter-rouge">pull</code>. Squash and merge is a
convenient button when needed, but the need is so infrequent that its only
mention in Git proper is the <code class="language-plaintext highlighter-rouge">squash</code> verb in the lexicon of interactive rebase.</p>

<p>You tread this route at your own peril.</p>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="git" /><category term="rants" /><summary type="html"><![CDATA[Squash and merge is a bad default action, and it grates when it’s the only permitted merge option.]]></summary></entry><entry><title type="html">Em-dashes are not spaced hyphens</title><link href="https://benknoble.github.io/blog/2024/06/16/em-dashes/" rel="alternate" type="text/html" title="Em-dashes are not spaced hyphens" /><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/06/16/em-dashes</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/06/16/em-dashes/"><![CDATA[<p>Please use an em-dash—like this one—for parenthetical asides or for a gentler
colon. <s>Do not use spaced hyphens - like this - they are wrong.</s></p>

<p>I don’t know where this trend came from or how to stop it. I suspect</p>

<ul>
  <li>Poor education for effective punctuation and grammar means fewer people know
about the <a href="https://practicaltypography.com/hyphens-and-dashes.html">different kinds of
dashes</a>.</li>
  <li>Keyboard and editor interfaces hide away <a href="https://practicaltypography.com/type-composition.html">various typographic
characters</a>, including
en- and em- dashes, making them less discoverable.</li>
  <li>In the twist of self-reinforcing trends, publicly visible uses make this
mistake more acceptable.</li>
</ul>

<p>I can’t top Butterick on explanations, so please review <a href="https://practicaltypography.com/hyphens-and-dashes.html">Hyphens and
Dashes</a> for the
different kinds and uses. I’m going to mention some input methods he omitted:</p>

<ul>
  <li>On an iPhone keyboard, holding the hyphen gives you options for dashes of
different lengths, including en- and em- dashes.</li>
  <li>(Programmers) In
<a href="https://jekyllrb.com/docs/configuration/markdown/#kramdown">Jekyll</a>- or
<a href="https://kramdown.gettalong.org/syntax.html#typographic-symbols">Kramdown</a>-
processed Markdown, the code <code class="language-plaintext highlighter-rouge">---</code> becomes an em-dash and <code class="language-plaintext highlighter-rouge">--</code> becomes an
en-dash, thanks to <a href="https://practicaltypography.com/typewriter-habits.html">Typewriter
Habits</a>. Note that
they must appear within text not to be confused with the horizontal rule
shorthand.</li>
  <li>(Programmers) In Vim, the digraphs <code class="language-plaintext highlighter-rouge">-N</code> and <code class="language-plaintext highlighter-rouge">-M</code> (enter with
<kbd>Ctrl</kbd>+<kbd>k</kbd>) produce en- and em- dashes, respectively.</li>
</ul>

<h2 id="collected-examples">Collected examples</h2>

<p>I accept submissions.</p>

<ul>
  <li>
    <p>A <a href="https://web.archive.org/web/20240530222028/https://fortellergames.com/blogs/news/short-and-long-term-app-plans">Foreteller Companion App post</a> reads:</p>

    <blockquote>
      <ul>
        <li>Growing concerns on regulations and legal ramifications - which trickle down into new policies to adhere to and shorter deadlines to implement, which puts our product development on hold</li>
        <li>App Store presence doesn’t help our niche audience - while ASO is important in most industries, we have qualitative data that shows overwhelmingly our installs come from direct installs from links, QRs, conventions</li>
      </ul>
    </blockquote>
  </li>
</ul>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="rants" /><category term="typography" /><summary type="html"><![CDATA[Please use an em-dash—like this one—for parenthetical asides or for a gentler colon. Do not use spaced hyphens - like this - they are wrong.]]></summary></entry><entry><title type="html">Re: The Functional Programming Hiring Problem</title><link href="https://benknoble.github.io/blog/2024/06/16/fp-hiring/" rel="alternate" type="text/html" title="Re: The Functional Programming Hiring Problem" /><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/06/16/fp-hiring</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/06/16/fp-hiring/"><![CDATA[<p>The identified problem may be missing the mark.</p>

<p><a href="https://blog.janissary.xyz/posts/hiring-functional-programming">Noah Snelson articulates a common theme of functional programming (FP)
discussions</a>:
the (dis)advantages of hiring engineers for less-well-known FP languages. I
personally appreciated the time Snelson spent crafting their article, but I’ve
got one issue with it: I think they missed a class of candidates.</p>

<p>Snelson believes that when hiring for languages such as the fictional Gooby, you
have 3 pools of applicants:</p>

<ol>
  <li>The résumé-spammers.</li>
  <li>Bright undergrads recently converted to the “One True Way.”</li>
  <li>Senior engineers who only care about Gooby.</li>
</ol>

<p>I agree that these are 3 types of potential Gooby applicants! I disagree that
there’s a meaningful difference between converted undergraduates and senior
engineers <em>in the context of the article’s problems.</em></p>

<h2 id="are-these-senior-engineers-behaving-like-senior-engineers">Are these senior engineers behaving like senior engineers?</h2>

<p>The senior engineers certainly have more Gooby expertise than the converted
undergraduates. Snelson’s characterizations reveal that these engineers are
still drinking the Kool-Aid, though:</p>

<ul>
  <li>They “<em>insist</em> on using a functional language at every moment possible”
(emphasis original).</li>
  <li>They “threaten to quit if their next project wasn’t written in their language
of choice.”</li>
  <li>They appeal to the authority of
<a href="https://en.wikipedia.org/wiki/Benevolent_dictator_for_life">BDFL</a>s “as a kind
of trump-card argument in technical discussions.”</li>
</ul>

<p>This does not, in my mind, a senior engineer make.</p>

<h2 id="another-kind-of-engineer">Another kind of engineer</h2>

<p>All the noise of technical obsession obscures the missing 4th class of
applicants: those who have the nuance of thought we expect from leaders. Those
who understand that <a href="/about/#me">simple questions have complex answers</a>.</p>

<p>The obsessed-senior problem is not particular to FP, either. Languages that
encourage imperative or OO style  have the same type of zealots (JavaScript,
Java, C, C++, etc.). Rust has its own zealotry and is markedly harder to
classify. <a href="https://cs.brown.edu/courses/cs173/2012/Videos/2012-09-05/2012-09-05.m4v">Shriram Krishnamurthi argues that such classifications are not a
useful concept: the features of a language and how they interact provides a
better discussion
frame.</a>
(I recommend the <a href="https://cs.brown.edu/courses/csci1730/2012/Videos/">whole video
series</a>).</p>

<p>This raises the question: if you’re selecting for problematic “senior”
engineers, are you selecting right? The problems of zealotry are correctly
identified (my personal experience agrees), but maybe “not selecting for
zealots” doesn’t mean “not selecting for language experts.” Selecting for senior
Gooby engineers does not require relaxing criteria for critical thinking.</p>

<p>If your hiring process filters for “personnel issues” of the kind Snelson
describes, then fix the hiring process. Look for candidates who consider
multiple angles and respond “it depends.” <a href="https://www.hillelwayne.com/talks/ese/">“Anyone who is certain about what is
and isn’t true about software is probably
wrong.”</a></p>

<p>Snelson concludes by citing Cromwell and a Tumblr user as a reminder that
sometimes we are wrong. Ironically, the Tumblr post’s “it depends” aligns with
exactly my argument: hire developers who understand and appreciate nuance. <em>They
are probably still deep experts of some domain</em>. Nuance typically comes from
expertise.</p>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="fp" /><summary type="html"><![CDATA[The identified problem may be missing the mark.]]></summary></entry><entry><title type="html">Re: Beautiful Python Monsters</title><link href="https://benknoble.github.io/blog/2024/06/15/beautiful-python-monsters/" rel="alternate" type="text/html" title="Re: Beautiful Python Monsters" /><published>2024-06-15T00:00:00+00:00</published><updated>2024-06-15T00:00:00+00:00</updated><id>https://benknoble.github.io/blog/2024/06/15/beautiful-python-monsters</id><content type="html" xml:base="https://benknoble.github.io/blog/2024/06/15/beautiful-python-monsters/"><![CDATA[<p>Making flaky tests pass cries out for functional-programming idioms.</p>

<p>While reading <a href="https://treyhunner.com/2024/06/a-beautiful-python-monstrosity/">A beautiful Python
monstrosity</a>, I
was struck by several thoughts. Perhaps performance tests shouldn’t be automated
with the same tools as unit tests? Is there a better way to write flaky tests?</p>

<h2 id="automatic-performance-tests">Automatic performance tests</h2>

<p>A unit test doesn’t seem to be the best way to describe a performance test. More
likely, we want to set up a benchmarking harness, run the programs, and measure
various performance indicators. <a href="https://github.com/drym-org/qi/blob/070ffc5e0d2e3a581a1bc11acd391e980dbdd328/.github/workflows/benchmarks.yml">This can be done automatically in a build
pipeline</a>,
for example.</p>

<p>If these measurements are collected on each run we can perform analyses. For
example:</p>

<ul>
  <li>Graph each run (with suitable titles) or make a <a href="https://drym-org.github.io/qi/benchmarks/">trend
analysis</a>.</li>
  <li>Perform <a href="https://chelseatroy.com/2021/02/26/data-safety-for-programmers/">statistical
tests</a>
between runs to see if the change was meaningful.</li>
</ul>

<p>All of this can also be automated. I wouldn’t write any of it as a unit test,
however.</p>

<p>The examples in the original post also seem to concentrate on algorithmic (time)
complexity; this can be done with statistical tests, too, given a wealth of
data. It’s not something I want to run as part of the (local) unit tests,
though. If it’s important enough to block PRs, put it in the build pipeline (and
I should be capable of running it locally; it shouldn’t be on by default,
though).</p>

<h2 id="flaky-tests">Flaky tests</h2>

<p>An orthogonal concept: How do we make flaky tests less flaky? We should engineer
the tests to be less reliant on flakiness, but automatically repeating the tests
is a reasonable hack in the meantime.</p>

<p>Here’s my transcribed Racket <code class="language-plaintext highlighter-rouge">repeat-flaky</code> procedure, which doesn’t require
decorators or other ideas:</p>

<div class="language-racket highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">define</span> <span class="p">(</span><span class="nf">repeat-flaky</span> <span class="nv">test</span> <span class="p">[</span><span class="nf">n</span> <span class="mi">10</span><span class="p">])</span>
  <span class="p">(</span><span class="nf">match</span> <span class="nv">n</span>
    <span class="p">[</span><span class="nf">0</span> <span class="p">(</span><span class="nf">test</span><span class="p">)]</span>
    <span class="p">[(</span><span class="nf">?</span> <span class="nv">positive?</span> <span class="nv">m</span><span class="p">)</span>
     <span class="p">(</span><span class="k">with-handlers</span> <span class="p">([</span><span class="nb">exn:fail?</span> <span class="p">(</span><span class="k">λ</span> <span class="p">(</span><span class="nf">_exn</span><span class="p">)</span> <span class="p">(</span><span class="nf">repeat-flaky</span> <span class="nv">test</span> <span class="p">(</span><span class="nb">sub1</span> <span class="nv">n</span><span class="p">)))])</span>
       <span class="p">(</span><span class="nf">test</span><span class="p">))]))</span>

<span class="c1">;; Example:</span>
<span class="p">(</span><span class="nf">repeat-flaky</span> <span class="p">(</span><span class="nf">thunk</span> <span class="p">(</span><span class="nf">check-equal?</span> <span class="p">(</span><span class="nb">random</span> <span class="mi">1</span> <span class="mi">3</span><span class="p">)</span> <span class="p">(</span><span class="nb">random</span> <span class="mi">1</span> <span class="mi">3</span><span class="p">))))</span>
</code></pre></div></div>

<p>The core idea is to pass functions around. Python makes this hard because its
<code class="language-plaintext highlighter-rouge">lambda</code> doesn’t permit arbitrary functions; the decorator over an unnamed
function is essentially recreating higher-order functions receiving anonymous
functions. This is thus the essence of repeating fallible tests; a
generalization allows the <code class="language-plaintext highlighter-rouge">exn:fail?</code> test to be replaced by the client or for
subsequent invocations to know about the caught exception by having it passed as
an optional argument.</p>

<p>The <code class="language-plaintext highlighter-rouge">thunk</code> could be eliminated (from surface syntax) by a macro if desired.</p>

<p>This works without needing <code class="language-plaintext highlighter-rouge">nonlocal</code>, by the way, since the Racket equivalent
of Python’s <code class="language-plaintext highlighter-rouge">a = 1</code> that automatically introduces <code class="language-plaintext highlighter-rouge">a</code> is <code class="language-plaintext highlighter-rouge">(let ([a 1]) …)</code> or
<code class="language-plaintext highlighter-rouge">(define a 1)</code>. This isn’t mutation, it’s binding. Further, if you want to refer
to <code class="language-plaintext highlighter-rouge">a</code> from a higher scope, you do so by writing <code class="language-plaintext highlighter-rouge">a</code> (as long it isn’t
shadowed), even with <code class="language-plaintext highlighter-rouge">set!</code>. Lexical scoping rules give you predictable control
of which identifiers refer to which bindings in each part of the program text.</p>

<p>The ability to properly nest <code class="language-plaintext highlighter-rouge">repeat-flaky</code> makes a lot of the need for mutation
(and thus complicated scope references) go away, though:</p>

<div class="language-racket highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="nf">repeat-flaky</span>
  <span class="p">(</span><span class="nf">thunk</span>
    <span class="p">(</span><span class="k">define</span> <span class="nv">micro-time</span> <span class="nv">…</span><span class="p">)</span>
    <span class="p">(</span><span class="k">define</span> <span class="nv">tiny-time</span> <span class="nv">…</span><span class="p">)</span>
    <span class="p">(</span><span class="nf">check</span> <span class="nv">…</span> <span class="nv">micro-time</span> <span class="nv">tiny-time</span> <span class="nv">…</span><span class="p">)</span>
    <span class="p">(</span><span class="nf">repeat-flaky</span>
      <span class="p">(</span><span class="nf">thunk</span>
        <span class="p">(</span><span class="k">define</span> <span class="nv">small-time</span> <span class="nv">…</span><span class="p">)</span>
        <span class="c1">;; we have access to micro, tiny here</span>
        <span class="p">(</span><span class="nf">check</span> <span class="nv">…</span> <span class="nv">micro-time</span> <span class="nv">tiny-time</span> <span class="nv">small-time</span> <span class="nv">…</span><span class="p">)))))</span>
</code></pre></div></div>

<p>Each nesting is repeated, however, so this singly-nested flaky test could run up
to 25 times (instead of the original’s 5). This could be a feature, though, and
can be controlled with the optional repeat argument.</p>

<h2 id="conclusions">Conclusions</h2>

<ol>
  <li>Take a hard look at what your goals for performance tests are. The original
post wanted them to predictably, consistently pass and fail like unit tests
and run quickly. I’m not sure that’s the best methodology for comparing
performance or for testing algorithmic time complexity.</li>
  <li>The essence of repeating fallible tests can be implemented by higher order
functions. An abuse of Python’s decorators allows Python to pass unnamed
functions to higher-order functions, working around a language deficiency.
Python’s <code class="language-plaintext highlighter-rouge">lambda</code> is not.</li>
</ol>]]></content><author><name>D. Ben Knoble</name></author><category term="[&quot;Blog&quot;]" /><category term="python" /><category term="racket" /><category term="fp" /><summary type="html"><![CDATA[Making flaky tests pass cries out for functional-programming idioms.]]></summary></entry></feed>